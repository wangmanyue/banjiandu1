{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb17019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\py\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('..')\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get rid of the deprecation warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from models.models import DeepStatisticalSolver\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768497e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this short method to compute the flows based on the heads\n",
    "def compute_flows(A, B, H):\n",
    "    \n",
    "    A_offset = A \\\n",
    "        + np.shape(B)[1] \\\n",
    "        * np.reshape(np.arange(np.shape(A)[0]), [-1, 1, 1]) \\\n",
    "        * np.array([[[1., 1., 0.]]])\n",
    "\n",
    "    \n",
    "    i_from = A_offset[:, :, 0].astype(np.int32)\n",
    "    i_to = A_offset[:, :, 1].astype(np.int32)\n",
    "    A_ij = A_offset[:, :, 2].astype(np.float32)\n",
    "    \n",
    "    elevation = B[:, :, 3]\n",
    "    Nd = B[:, :, 0]\n",
    "    Nh = B[:, :, 2]\n",
    "    elevation = np.reshape(elevation, -1)\n",
    "    Nd = np.reshape(Nd, -1)\n",
    "    Nh = np.reshape(Nh, -1)\n",
    "    \n",
    "    H_i = H[i_from]*Nd[i_from] + (1-Nd[i_from]) * elevation[i_from]\n",
    "    H_j = H[i_to]*Nd[i_to]  + (1-Nd[i_to]) * elevation[i_to]\n",
    "\n",
    "    \n",
    "    H_ij = H_i - H_j\n",
    "    Q_ij = np.sign(H_ij)* (np.maximum(np.abs(H_ij),1e-9)*A_ij)**(1/1.852)\n",
    "\n",
    "    return Q_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0771e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateMetrics(model, A_np, B_np, U_np, sess):\n",
    "    # For the whole dataset, we will compute both the individual loss of each sample, \n",
    "    # and the final predictions\n",
    "    costs_DSS = None\n",
    "    costs_NR = None\n",
    "\n",
    "    Vm_DSSs = None\n",
    "    Va_DSSs = None\n",
    "    Vm_NRs = None\n",
    "    Va_NRs = None\n",
    "\n",
    "    P_ij_DSSs = None\n",
    "    Q_ij_DSSs = None\n",
    "    P_ij_NRs = None\n",
    "    Q_ij_NRs = None\n",
    "\n",
    "    # In order to split the dataset, define the size of the batches that will be fed to the model.\n",
    "    # For very large dataset, it can be useful to have a small value for BATCH_SIZE\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    # Get total amount of samples and split the dataset\n",
    "    n_samples_tot = A_np.shape[0]\n",
    "    n_batches = n_samples_tot // BATCH_SIZE\n",
    "    batched_indices = np.array_split(np.arange(n_samples_tot), n_batches) \n",
    "\n",
    "    # Iterate over the batches\n",
    "    for indices in tqdm.tqdm(batched_indices):\n",
    "\n",
    "        U_DSS, cost_DSS = sess.run([model.U_final, model.cost_per_sample[str(model.correction_updates)]], \n",
    "                 feed_dict = {model.A:A_np[indices], model.B:B_np[indices]}\n",
    "                )\n",
    "\n",
    "        cost_NR = sess.run(model.cost_per_sample[str(model.correction_updates)], \n",
    "                 feed_dict = {model.A:A_np[indices], model.B:B_np[indices], model.U_final:U_np[indices]}\n",
    "                )\n",
    "\n",
    "        # Getting predicted HEAD\n",
    "        H_DSS = U_DSS[:, :, 0]\n",
    "        H_DSS = np.reshape(H_DSS, -1)\n",
    "\n",
    "        # Getting actual HEAD\n",
    "        H_NR = U_np[indices, :, 0]\n",
    "        H_NR = np.reshape(H_NR, -1)\n",
    "           \n",
    "\n",
    "        Q_ij_DSS = compute_flows(A_np[indices], \n",
    "                                             B_np[indices], \n",
    "                                             H_DSS)\n",
    "\n",
    "        Q_ij_NR = compute_flows(A_np[indices], \n",
    "                                         B_np[indices], \n",
    "                                         H_NR)\n",
    "\n",
    "        Q_ij_DSS = np.reshape(Q_ij_DSS, -1)\n",
    "        Q_ij_NR = np.reshape(Q_ij_NR, -1)\n",
    "\n",
    "\n",
    "        \n",
    "        if costs_DSS is None:\n",
    "            costs_DSS = cost_DSS\n",
    "            costs_NR = cost_NR\n",
    "            H_DSSs = H_DSS\n",
    "            H_NRs = H_NR\n",
    "            Q_ij_DSSs = Q_ij_DSS\n",
    "            Q_ij_NRs = Q_ij_NR\n",
    "\n",
    "        else:\n",
    "            costs_DSS = np.concatenate([costs_DSS, cost_DSS])\n",
    "            costs_NR = np.concatenate([costs_NR, cost_NR])\n",
    "            H_DSSs = np.concatenate([H_DSSs, H_DSS])\n",
    "            H_NRs = np.concatenate([H_NRs, H_NR])\n",
    "            Q_ij_DSSs = np.concatenate([Q_ij_DSSs, Q_ij_DSS])\n",
    "            Q_ij_NRs = np.concatenate([Q_ij_NRs, Q_ij_NR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba46595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateMetrics(model, A_np, B_np, U_np, sess):\n",
    "    # For the whole dataset, we will compute both the individual loss of each sample, \n",
    "    # and the final predictions\n",
    "    costs_DSS = None\n",
    "    costs_NR = None\n",
    "\n",
    "    Vm_DSSs = None\n",
    "    Va_DSSs = None\n",
    "    Vm_NRs = None\n",
    "    Va_NRs = None\n",
    "\n",
    "    P_ij_DSSs = None\n",
    "    Q_ij_DSSs = None\n",
    "    P_ij_NRs = None\n",
    "    Q_ij_NRs = None\n",
    "\n",
    "    # In order to split the dataset, define the size of the batches that will be fed to the model.\n",
    "    # For very large dataset, it can be useful to have a small value for BATCH_SIZE\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    # Get total amount of samples and split the dataset\n",
    "    n_samples_tot = A_np.shape[0]\n",
    "    n_batches = n_samples_tot // BATCH_SIZE\n",
    "    batched_indices = np.array_split(np.arange(n_samples_tot), n_batches) \n",
    "\n",
    "    # Iterate over the batches\n",
    "    for indices in tqdm.tqdm(batched_indices):\n",
    "\n",
    "        U_DSS, cost_DSS = sess.run([model.U_final, model.cost_per_sample[str(model.correction_updates)]], \n",
    "                 feed_dict = {model.A:A_np[indices], model.B:B_np[indices]}\n",
    "                )\n",
    "\n",
    "        cost_NR = sess.run(model.cost_per_sample[str(model.correction_updates)], \n",
    "                 feed_dict = {model.A:A_np[indices], model.B:B_np[indices], model.U_final:U_np[indices]}\n",
    "                )\n",
    "\n",
    "        # Getting predicted HEAD\n",
    "        H_DSS = U_DSS[:, :, 0]\n",
    "        H_DSS = np.reshape(H_DSS, -1)\n",
    "\n",
    "        # Getting actual HEAD\n",
    "        H_NR = U_np[indices, :, 0]\n",
    "        H_NR = np.reshape(H_NR, -1)\n",
    "           \n",
    "\n",
    "        Q_ij_DSS = compute_flows(A_np[indices], \n",
    "                                             B_np[indices], \n",
    "                                             H_DSS)\n",
    "\n",
    "        Q_ij_NR = compute_flows(A_np[indices], \n",
    "                                         B_np[indices], \n",
    "                                         H_NR)\n",
    "\n",
    "        Q_ij_DSS = np.reshape(Q_ij_DSS, -1)\n",
    "        Q_ij_NR = np.reshape(Q_ij_NR, -1)\n",
    "\n",
    "\n",
    "        \n",
    "        if costs_DSS is None:\n",
    "            costs_DSS = cost_DSS\n",
    "            costs_NR = cost_NR\n",
    "            H_DSSs = H_DSS\n",
    "            H_NRs = H_NR\n",
    "            Q_ij_DSSs = Q_ij_DSS\n",
    "            Q_ij_NRs = Q_ij_NR\n",
    "\n",
    "        else:\n",
    "            costs_DSS = np.concatenate([costs_DSS, cost_DSS])\n",
    "            costs_NR = np.concatenate([costs_NR, cost_NR])\n",
    "            H_DSSs = np.concatenate([H_DSSs, H_DSS])\n",
    "            H_NRs = np.concatenate([H_NRs, H_NR])\n",
    "            Q_ij_DSSs = np.concatenate([Q_ij_DSSs, Q_ij_DSS])\n",
    "            Q_ij_NRs = np.concatenate([Q_ij_NRs, Q_ij_NR])\n",
    "            \n",
    "    print('Loss - GNS')\n",
    "    print('    10th percentile = {}'.format(np.percentile(costs_DSS, 10)))\n",
    "    print('    50th percentile = {}'.format(np.percentile(costs_DSS, 50)))\n",
    "    print('    90th percentile = {}'.format(np.percentile(costs_DSS, 90)))\n",
    "    print('Loss - NR')\n",
    "    print('(Keep in mind that the maximum precision achieved by tf.float32 is around 1e-14')\n",
    "    print('Thus the loss computed by tensorflow is noisy)')\n",
    "    print('    10th percentile = {}'.format(np.percentile(costs_NR, 10)))\n",
    "    print('    50th percentile = {}'.format(np.percentile(costs_NR, 50)))\n",
    "    print('    90th percentile = {}'.format(np.percentile(costs_NR, 90)))\n",
    "    print('Correlation between methods')\n",
    "    print('    Correlation H = {}'.format(np.corrcoef(H_DSSs,H_NRs)[1,0]))\n",
    "    print('    Correlation Qij = {}'.format(np.corrcoef(Q_ij_DSSs,Q_ij_NRs)[1,0]))\n",
    "    print('RMSE between methods')\n",
    "    print('    RMSE H = {}'.format(np.sqrt(np.mean((H_DSSs-H_NRs)**2))))\n",
    "    print('    RMSE Qij = {}'.format(np.sqrt(np.mean((Q_ij_DSSs-Q_ij_NRs)**2))))\n",
    "    print('normalized RMSE between methods')\n",
    "    print('    normalized RMSE H = {}'.format(np.sqrt(np.mean((H_DSSs-H_NRs)**2))\\\n",
    "          /(np.max(H_NRs)-np.min(H_NRs))))\n",
    "    print('    normalized RMSE Qij = {}'.format(np.sqrt(np.mean((Q_ij_DSSs-Q_ij_NRs)**2))\\\n",
    "          /(np.max(Q_ij_NRs)-np.min(Q_ij_NRs))))\n",
    "    print('MAE between methods')\n",
    "    print('    MAE H = {}'.format(np.mean(np.abs(H_DSSs-H_NRs))))\n",
    "    print('    MAE Qij = {}'.format(np.mean(np.abs(Q_ij_DSSs-Q_ij_NRs))))\n",
    "    print('normalized MAE between methods')\n",
    "    print('    normalized MAE H = {}'.format(np.mean(np.abs(H_DSSs-H_NRs))\\\n",
    "          /(np.max(H_NRs)-np.min(H_NRs))))\n",
    "    print('    normalized MAE Qij = {}'.format(np.mean(np.abs(Q_ij_DSSs-Q_ij_NRs))\\\n",
    "          /(np.max(Q_ij_NRs)-np.min(Q_ij_NRs))))\n",
    "\n",
    "    return H_NRs, H_DSSs, Q_ij_NRs, Q_ij_DSSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48c7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"results/1716657662\" # supervised trained on asnet2_1, k=l=20,no scaling on H loss, initalized at 399\n",
    "name='supervised'\n",
    "# Enter path to data to build architecture\n",
    "data_dir = 'datasets/asnet38'\n",
    "\n",
    "# Initialize a tensorflow session\n",
    "sess = tf.Session()\n",
    "# Build the Deep Statistical Solver\n",
    "model = DeepStatisticalSolver(sess, \n",
    "                          model_to_restore=model_path, \n",
    "                          default_data_directory=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c2dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - GNS\n",
      "    10th percentile = 0.0006482791621237994\n",
      "    50th percentile = 0.0006787704769521952\n",
      "    90th percentile = 0.0007520776474848389\n",
      "Loss - NR\n",
      "(Keep in mind that the maximum precision achieved by tf.float32 is around 1e-14\n",
      "Thus the loss computed by tensorflow is noisy)\n",
      "    10th percentile = 0.042254763841629024\n",
      "    50th percentile = 0.044397417455911636\n",
      "    90th percentile = 0.04686950892210007\n",
      "Correlation between methods\n",
      "    Correlation H = 0.06528781732114461\n",
      "    Correlation Qij = 0.20681747233205708\n",
      "RMSE between methods\n",
      "    RMSE H = 0.6488629169642064\n",
      "    RMSE Qij = 0.06999599179095542\n",
      "normalized RMSE between methods\n",
      "    normalized RMSE H = 0.7009752776473616\n",
      "    normalized RMSE Qij = 0.09964104461606145\n",
      "MAE between methods\n",
      "    MAE H = 0.6315297457513281\n",
      "    MAE Qij = 0.03702263815327428\n",
      "normalized MAE between methods\n",
      "    normalized MAE H = 0.6822500212244755\n",
      "    normalized MAE Qij = 0.052702651189683904\n"
     ]
    }
   ],
   "source": [
    "# Load the test set (change the mode variable if you want val or train)\n",
    "data_dir = 'datasets/asnet38'\n",
    "mode = 'test'\n",
    "\n",
    "# Import numpy data\n",
    "A_np = np.load(os.path.join(data_dir, 'A_'+mode+'.npy'))\n",
    "B_np = np.load(os.path.join(data_dir, 'B_'+mode+'.npy'))\n",
    "U_np = np.load(os.path.join(data_dir, 'U_'+mode+'.npy'))\n",
    "# coord_np = np.load(os.path.join(data_dir, 'coord_'+mode+'.npy'))\n",
    "H_NRs, H_DSSs, Q_ij_NRs, Q_ij_DSSs = CalculateMetrics(model, A_np, B_np, U_np, sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_min_max = [np.min(H_NRs), np.max(H_NRs)]\n",
    "Q_min_max = [np.min(Q_ij_NRs), np.max(Q_ij_NRs)]\n",
    "fig,axs = plt.subplots(1,2, figsize = (8,4))\n",
    "axs[0].plot(H_NRs, H_DSSs,'k.')\n",
    "axs[0].plot(H_min_max,H_min_max, 'r-')\n",
    "axs[0].set_xlabel('EPANET  [m]')\n",
    "axs[0].set_ylabel('Supervised GNN  [m]');\n",
    "axs[1].plot(Q_ij_NRs, Q_ij_DSSs,'k.')\n",
    "axs[1].plot(Q_min_max,Q_min_max, 'r-')\n",
    "axs[1].set_xlabel('EPANET flows [$m^3/s$]')\n",
    "axs[1].set_ylabel('Supervised GNN flows [$m^3/s$]')\n",
    "plt.tight_layout()\n",
    "fig.savefig('correlation_'+name+'.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2fae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
